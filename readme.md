# Introduction to using snpArcher on Della

NOTE: all credit to Brian Arnold -- this is adapted from his excellent ["Intro to Comp Bio Workflows" workshop](https://github.com/brian-arnold/intro_compbio_workflows_2024) he gave here in early 2024. If you're just getting started working on Della or doing bioinformatics, I highly recommend looking through all of the readmes in his workshop -- it's a great intro! snpArcher has been updated since Brian gave this workshop. These updates broke some things, so I've modified the instructions for day 3 of his workshop to work as of April 2025. 

These instructions assume you have some general knowledge of the Della cluster, using conda, scripting, and command line tools, as well as the general concept behind snpArcher and what it's used for. If you don't, days 1 and 2 of Brian's workshop will be very helpful with getting you up to speed. Additionally, take a look at [the snpArcher paper](https://academic.oup.com/mbe/article/41/1/msad270/7466717)!

### Getting a reference genome

To run snpArcher you need sequencing data and a reference genome to map it to! Just a few notes on finding and using reference genomes and published sequencing data.

An important thing to note before starting: one of Della's (many) interesting quirks is that is doesn't have internet access on the compute nodes. What this means for us is that, even though snpArcher has the capability to pull data from e.g. for you, it won't be able to. Instead, all of our data -- reference genome, generated sequencing data and published sequencing data -- must be downloaded locally to Della before we can run snpArcher. 

I have a directory in my scratch called `reference_genomes` where I store well, reference genomes. To find the one for your species, you can go to the [NCBI Genomes resource](https://www.ncbi.nlm.nih.gov/home/genomes/) and type in the scientific name of your species in the search bar. Or, a quick google of "species name genome ncbi" usually does the trick. 

I tend to download reference genomes using ftp. Is this the best/fastest/most used way? I don't know, honestly! Can anyone comment?

Once you're on the page of the genome assembly you'd like to download (I choose the genome with the green "reference" tag and the green checkmark), click on "ftp" at the top of the page. Find the link ending in .fna.gz and copy this link -- this the ftp (file transfer protocol) for your genome. The other stuff here might link to an annotation file, QC files for the assembly, coding sequences files, RNA transcript sequences, etc. You might need these later for other analyses, but we don't need them for snpArcher.

Once you have the ftp link to the reference genome you want to download copied, navigate to the directory where you're going to put your reference genome on Della. 

`cd /scratch/gpfs/ml9889/reference_genomes/genomes`

Then you can just use wget to pull this genome using the ftp link. (Check: I think wget is automatically installed in your base environment?)

`wget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/028/564/725/GCA_028564725.2_OtoMan_p1.0/GCA_028564725.2_OtoMan_p1.0_genomic.fna.gz`

All of my genomes are unzipped -- I don't know why. To unzip yours you can use this command:

`gunzip GCA_028564725.2_OtoMan_p1.0_genomic.fna.gz`

Now your reference genome is on Della.

### Pulling data from a sequence repository

You will probably have sequencing data you've generated that you want to map to this reference genome. You'll get instructions for downloading your sequencing data from whatever sequencing core sequenced your samples. I suggest backing this data up as soon as you get it, since scratch is not backed up!

You may also want to find some sequencing data generated by other people and published in some sequencing repository like NCBI, ENA, etc. More data can be helpful, because it increases the power of the mapping and calling algorithms that snpArcher uses.

You might want to take several different approaches for looking for published data. One approach is to just search on NCBI for published sequences in the Sequence Read Archive (SRA). If there is a lot of published data for your species, it might be easier to sort through by clicking on the "Send to Run Selector" link at the top of the SRA page for the species of interest. This gives you a table that might be easier to sort and navigate. 

Use the check boxes to select the specific runs you want to download. There may be several per sample or experiment, is some individuals were sequenced multiple times or different libraries were prepared. 

The data in SRA are laid out like
    - SRP is Study, which can have ≥1 sample
    - SRS is Sample, which can have ≥1 experiment
    - SRX is Experiment, which can have ≥1 run
    - SRR is Run. These are where the actual DNA sequences are!

Once you have some data you want to download, you can click the "Accession List" option under "Selected Data". This will give you a text file with a list of accession numbers for the runs you selected. 

From here you'll need to use the [SRA-toolkit](https://github.com/ncbi/sra-tools/wiki) to download the data. (Check: What do other people do?). This is because NCBI stores sequencing data in this special SRA format. You can use Globus to transfer data from other sources like ENA, or you can always wget ftp links. 

I store my sequencing data in another subdirectory called `raw_seq_data`, organized into further subdirectories.

### Setting up main snparcher environment

FYI, the snpArcher [docs](https://snparcher.readthedocs.io/en/latest/setup.html) are good! The github "issues" page is also an excellent resource if you ever run into problems. And, if you need to ask a question, the guys who are currently developing snpArcher seem really nice and helpful. 

Once you have a reference genome to map to and some sequencing data, you're ready to begin setting up snpArcher itself!

The first thing the setup guide in the docs recommends is making a new snparcher conda environment with:

`mamba create -c conda-forge -c bioconda -n snparcher "snakemake>=8" "python==3.11.4"`

In Brian's tutorial he suggests then doing the following, to circumvent an error

`mamba install -n snparcher -c conda-forge mamba`

If you've used snpArcher in the past, you won't need to do this again the next time you use it. 

Then, you'll need to clone the snpArcher github repo, which contains the pipeline and code. Do this with

`git clone https://github.com/harvardinformatics/snpArcher.git`

Check: For some reason, I've always cloned a new repo for each project. I can't remember if I do this for a reason, or if I just started doing it and now I still am. Can anyone comment?

### snpArcher configuration 

You will need to get a couple of things set up prior to running snpArcher.

1. The most important is creating a samples.csv file, which you'll need to add to the `config` subdirectory. This gives the pipeline all of the metadata it needs to run. Some tips:

    - There is a script to create this for you, but I find it's only suitable in a few situations. I almost always make mine at least semi by hand. 

    - See the docs for the required columns. You will want to specify absolute paths.

    - I don't recommend changing the name of this file -- just call it `samples.csv` and stick it in the `config` subdirectory

2. The `config.yaml` file in this `config` subdirectory contains a lot of parameters you can adjust. Brian's workshop suggests just leaving most of these at the default. Here are some things you may want to change though:

    - `final_prefix` -- this is the prefix your results files will have

    - The pipeline was originally written for low coverage data, so the low coverage options will be selected by default. If you're working with data that has higher coverage than say, 10X, you can comment these lines out and uncomment the one for high coverage data

    - I remove the string in `scaffolds_to_exclude` 

3. The biggest changes since Brian's workshop are in the slurm config file, which has been condensed (previously this info was in `resources.yaml` and the slurm `config.yaml`). You can find this "new" file at `profiles/slurm/config.yaml`. These updates added a lot of options -- this improved the pipeline a lot but also means there is more to do to get it set up.

    - you may want to set `retries` to something like 3. If any step errors out because of memory, the pipeline will resubmit that step with more memory and try again, up to the number of times you specify here. 

    - I set `deafult resources` memory to 4000 (both `mem_mb` and `mem_mb_reduced`) and `runtime` to a day 1440 minutes. The higher you set this the less likely you are to fail steps because of timeouts or OOM errors. But, the higher you set this the longer your jobs will sit in the queue. 

    - For threads, I copied over the resources Brian specified in the old resources file and have made the following changes
        - genmap: 10
        - get_fastq_pe: 8 
        - fastp: 8
        - bwa_map: 20
        - dedup: 20
        - gvcf2DB: 2 
        - DB2vcf: 2
        - filterVcfs: 2
        - sort_gatherVcfs: 2
        - compute_d4: 4
    
    - You can set resources for specific rules after this. These will all come commented out as the default. You should leave anything you aren't changing commented. I made a lot of changes here. This is the part that took a lot of trial and error. To start you will need to uncomment the `set-resources:` line. If you want to change any resources for a rule, you need to uncomment the rule line, e.g. `index_reference:` and then whatever you're changing e.g. `mem_mb: attempt * 20000`. If you're not changing the slurm partition (you aren't) or the runtime, leave these lines commented. The changes I made are too lengthy to detail here, but see the `slurm_config.yaml` file for what I've used in the past. You may need to tweak these depending on how much data you have, how big the genome is, etc. Note that this file should be named `config.yaml` and it should be in `profiles/slurm`. 

4. Be sure to save all of these files: `sample.csv`, `config/config.yaml` and `slurm/config.yaml` before running anything! 

### More Della specific set up

Copying the rest of this is pretty much directly from Brian, with only minor modifications. Brian provided us with three scripts: One for setting up environments on Della, one for a dry run, and one for actually submitting the main snparcher job. The versions here have been modified by me to make them work after the updates. Okay, here's Brian:

Snakemake can create it's own mamba environments, and snpArcher automatically installs all the relevant software specified in `snpArcher/workflow/envs`. However, Della has a quirk in that compute nodes *do not* have access to internet, which is needed to download software. Thus, conda environments need to be created on login nodes, which *do* have internet, before we run anything.

1. In case it takes a while and you need to close your computer, let's make a 'screen' using `screen -S smk` that creates a seperate terminal instance

2. Now that we are in this separate screen, install conda environments using `bash 01_install_conda_envs.sh`. We may have to make some changes here, let's see. 

3. Press `ctrl+A` then `D` to 'detatch' from the session, which should run in the background even if you close your terminal window.

4. To go back into the screen to check proress, type `screen -r smk`, where `-r` is to 'resume.

5. When it's finally done, kill the screen by pressing `ctrl+A` then `K`, and respond yes.

If you type `mamba env list`, you should see a bunch of new environments were created in a hidden directory `.snakemake` that you can only see using `ls -a`. These environments don't have names like the ones we created by hand.

NOTE: You could also do this directly on the command line on the login node, but using screen allows you to close your computer and it continues the process.

MADISON'S NOTE: You have to do this for every snpArcher directory you have. Again, I have separate ones for each of my projects. This means I have a lot of these environments in the hidden `.snakemake` directory. 
        
### Dry run to test snpArcher

Next, let's do a dry run to see if we have everything configured correctly.

`bash 02_dry_run.sh`

If the dry run completes successfully and shows some rules that need to be run, then everything is good to go. However, the number of rules is shows that need to be run isn't the entire story, since some rules are executed only under some conditions that are determined at runtime. In practice, snparcher will likely run thousands of rules and submit thousands of jobs.

### Running snpArcher

Lastly, we can submit the last script `03_submit_smk_job.sh` as a job using the `sbatch` command, specifying the longest time interval possible to ensure it doesn't time out. This job that gets submitted will essentially submit many additional jobs, one for each rule that needs to get run. 

MADISON'S NOTE: I think the new version of snparcher will chastise you for submitting the main job as a slurm job, but to avoid using too many resources on the login node I'm not sure how else to do this. I know people on Argos just bash the main job, but idk if we'd get yelled at by Della for that...

Some more notes: 
- this may be a job to have an email set up to alert you if it errors out/finishes
- sometimes, if you have a lot of data, this may take more than 6 days to run. Make sure to plan your submission around the annoying della downtimes. And, if you need to add more time you can write to cses@princeton.edu and politely ask them to add X amount more time to your main job (give them the job id). 

Check: do ther people have tips to add here? 

### Interpreting the results 

Everything after here is just copy pasted from my personal notes! I'll make these more legible later! 

### (1) First, info about SNParcher outputs

The main SNParcher pipeline gives you a final vcf file that ends with raw.vcf.gz. This vcf contains all of the samples you input and variants (SNPs and indels) called. 

In the process of calling variants, very little is actually removed. Variant calling is intentionally lenient. It aims to maximize sansitivity (the chance you will pick up on a variant) at the expense of accuracy (you will probably call some variants with little evidence to support them being "real"). While you may include some false positives through this process, you reduce the risk of throwing out true positives.

#### SNParcher's process for generating the VCF

It's probably important to understand the steps that get you from BAM to this raw.vcf.gz so that you have a basic idea of what's going on at each step. The output of this multi-step process is a raw vcf file that you should consider filtering in different ways. As the SNParcher paper notes, this workflow is pretty much standard in the field. The default parameters are used in most of these steps, per GATK Best Practices.

**Step 0: Mapping**: 
First, note that the bwa-mem mapping stage does take into account mappability, and reads that align to regions of the genome with poor mappability will recieve lower mapping quality scores. These lower quality areas are not removed, as far as I can tell, just annotated.

**Step 1: Calling Variants**:
After mapping, GATK is used to call variants for each sample's mapped reads. SNParcher uses HaplotypeCaller for this process. (HaplotypeCaller is the recommended variant caller in almost all cases, according to GATK Best Practices, and it's the only caller that SNParcher uses, I think). HaplotypeCaller will call both SNPs and indels at the same time.

Nothing is "filtered" at this stage. Instead, calls are simply not made if there is not enough confidence for them. The SNParcher code for this step uses the GATK defaults for call confidence, which you can view [here, in the variant calling tutorial](https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/0471250953.bi1110s43). The output of this step is a gvcf, which includes reference confidence scores (support for the variant at that site), one for each sample. Parameters that also affect calls are `--min-pruning` and `--min-dangling-branch-length` which are semi-set in the config file. These are set when you decide whether to use the low-coverage pipeline (<10X) or the higher coverage one (>10X). 

Note that the process of calling variants occurs _per sample_. 

**Step 2: Calling Genotypes**:
The next step is to call genotypes and aggregate the gvcfs into a single vcf containing data from all samples. To make this effecient, SNParcher first uses GenomicsDBImport. I don't 100% understand what's going on here, but I don't think it matters too much. I think it's just organizing the computing work to be more efficient. 

GenotypeGVCFs is then used to do the actual calling. Again, at this step calls are just not made if there is not enough evidence to support them. Default call confidence thresholds are used (see the tutorial above, or ask chatGPT what they are). The one parameter that is used that you set is the prior probability of heterozygosity, which is set in the config file. The default value in the config (0.005) is higher that GATK's default (0.001). I don't really understand the implications of this, and should probably **ask Brian about this**. 

**Step 3: Annotating Called Variants with Filters**:
SNParcher then takes this vcf and "filters" it. I think the language used in a lot of papers I've read describing the processing methods makes this step seem different from what actually happens (or maybe my reading comprehension skills are poor, idk). At this step, at least in the SNParcher pipeline, nothing is actually removed. Instead, you're just flagging variants that don't pass certain filters, and you can actually remove them from your dataset later. Again, things aren't removed, just annotated.

GATK Best Practices are used here. 
- **RPRS_filter**: flags variants supported by reads aligned in suboptimal positions or have poor quality relative to their depth
    - for SNPs, `ReadPosRankSum` <-8.0 (for indels < -20.0)
    - for both SNPs and indels, anything with quality by depth (`QD`) < 2.0
- **FS_SOR_filter**: flags sites with strand bias (e.g. sequencing preferred the forward or reverse strand, so there is more skewed evidence for het or homozygosity compared to what you'd see if both strands were equally likely to be sequenced)
    - For SNPs, if Fisher Strand `FS` > 60.0 or Strand Odds Ratio `SOR` > 3.0
    - For indels or mixed sites, if `FS` > 200.0 or `SOR` > 10.0
- **MQ_filter**: annotates areas with low average mapping quality
    - For SNPs, if Mapping Quality `MQ` < 40.0 or `MQRankSum` < -12.5.
- **QUAL_filter**: annotates areas with low quality scores
    - `QUAL` less than 30.0

If a variant gets a filter flag it did not "pass" that filter. Variants with filter flags are ones you should consider removing, but again it's up to you to do this later. 

After all of this, you get a raw.vcf.gz


#### Optional additional filtering with SNParcher

If you don't set up your sample.csv and config to include the postprocessing module, SNParcher will stop after it generates the raw.vcf.gz. Totally fine, you can filter this further yourself with vcftools/bcftools. But if you want, SNParcher will also perform some more filtering if you include the postprocesisng module. I believe this is intended to be run after you've run the pipeline and got your raw.vcf.gz. You can also run it by default, because why not. Just note that you should probably filter the raw vcf manually, later, to your specifications.

To make the postprocessing module run, you need to include the SampleType column in the sample.csv file. In this column, put "include" for each row. You will also have to set the parameters for postprocessing in the config file in the **Filtering options** section. I just used the default parameters when I was running this.

Of course, if you've already run the pipeline and identified samples that are very low coverage, contaiminated, etc. with the QC module, you can set those samples to "exclude" in the samples.csv and they will be filtered out in the postprocessing module. 

Here are the steps the postprocessing module performs:

**Step 1: Basic Filtering**:
At this step, SNParcher does three things:
1. removes those samples that you told it to exclude in the sample.csv file, if any
2. filters out sites that were flagged above (ie, sites that don't have a . or PASS in the FILTER COLUMN)
    - in my vcf, no sites are marked with PASS. You can check the FILTER.summary in results for a summary of what was marked. I also ran the following on the command line to confirm:
    `zgrep -v "^#" 202405_AENP_GNP_NCBI_raw.vcf.gz | head -n 10000 | awk -F'\t' '$7 == "PASS" {count++} END {print count+0}'` This printed 0 for me.
    `zgrep -v "^#" 202405_AENP_GNP_NCBI_raw.vcf.gz | head -n 10000 | awk -F'\t' '$7 == "MQ_filter" {count++} END {print count+0}'` You should get something >0 for this one (unless your dataset is super good I guess?)
    `zgrep -v "^#" 202405_AENP_GNP_NCBI_raw.vcf.gz | head -n 10000 | awk -F'\t' '$7 == "." {count++} END {print count+0}'` Should definitely be >0, these are the unlabelled PASSes
3. removes from the vcf any sites that are not polymorphic, or where the ref genome is poor quality
    - sites that don't have any variation (all samples have the same sequence)
    - sites where the reference has an N or where the alternate is . (no alternate allele)
    - sites with allele frequency = 0 (no alternate alleles observed)

The output here is the filtered.vcf.gz file.

**STEP 2: Strict Filtering**:
Then SNParcher will use bcftools to filter according to the parameters you specify in the config file, and split the vcf into one for SNPs and one for indels. 
1. excludes variants in regions of the genome comprised of small contigs 
    - not entirely sure if I completely understand why you would want to exclude these but ¯\_(ツ)_/¯
2. removes variants with "missingness" above the threshold specified in the config.yaml
3. removes variants with a minor allele frequency below the threshold specified in the config.yaml
4. removes variants on chromosomes you specify in the config.yaml
    - for this step, I'm not sure if you need to name the chromosome as in the reference genome, e.g. for me to exclude the Y chromosome, would I need to put NC_087370.1?

The output of this is a TEMP file that is then just subsetted in the next snakemake rules. This TEMP file gets split into a SNP file and an indel file. Wish they had an option to keep the TEMP file with them together but oh well! 


