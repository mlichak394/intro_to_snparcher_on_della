# Introduction to using snpArcher on Della

NOTE: all credit to Brian Arnold -- this is adapted from his excellent ["Intro to Comp Bio Workflows" workshop](https://github.com/brian-arnold/intro_compbio_workflows_2024) he gave here in early 2024. If you're just getting started working on Della or doing bioinformatics, I highly recommend looking through all of the readmes in his workshop -- it's a great intro! snpArcher has been updated since Brian gave this workshop. These updates broke some things, so I've modified the instructions for day 3 of his workshop to work as of April 2025. 

These instructions assume you have some general knowledge of the Della cluster, using conda, scripting, and command line tools, as well as the general concept behind snpArcher and what it's used for. If you don't, days 1 and 2 of Brian's workshop will be very helpful with getting you up to speed. Additionally, take a look at [the snpArcher paper](https://academic.oup.com/mbe/article/41/1/msad270/7466717)!

###
## (1) Downloading a reference genome

To run snpArcher you need sequencing data and a reference genome to map it to. Just a few notes on finding and using reference genomes and published sequencing data.

**An very important thing to note:** one of Della's (many) interesting quirks is that is doesn't have internet access on the compute nodes. What this means for us is that, even though snpArcher has the capability to pull data from e.g. NCBI for you, it won't be able to. Instead, all of our data--reference genome, generated sequencing data and any published sequencing data you might want to use--must be downloaded locally to Della before we can run snpArcher. 

I have a directory in my scratch called `reference_genomes` where I store well, reference genomes. To find the reference genome for your species, you can go to the [NCBI Genomes resource](https://www.ncbi.nlm.nih.gov/home/genomes/) and type in the scientific name of your species in the search bar. Or, a quick google of "species name genome ncbi" usually does the trick. 

I download reference genomes using ftp:

1. On the NCBI page for your species look for a genome with a "reference" tag and/or a green checkmark. There may be other options besides the genome NCBI considers the current reference. These are probably older genome assemblies, they might be scaffold level or created with older technology. 
    
2. Click on think link for the assembly your want to download, then click on "ftp" at the top of the page. 

3. Find the link ending in .fna.gz and copy this link (right click and "copy link")
    - This the ftp (file transfer protocol) for your reference genome sequence. 
    - The other stuff here might link to an annotation file, QC files for the assembly, coding sequences files, RNA transcript sequences, etc. You might need these later for other analyses, but we don't need them for snpArcher.

4. Once you have the ftp link to the reference genome you want to download copied, navigate to the directory where you're going to put your reference genome on Della. 

    `cd /scratch/gpfs/ml9889/reference_genomes/genomes`

5. Use `wget` to pull this genome using the ftp link. (**Check:** I think wget is automatically installed in your base environment?). Here I'm pulling the genome for my favorite animal, the Pallas Cat (Otocolobus manul).

    `wget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/028/564/725/GCA_028564725.2_OtoMan_p1.0/GCA_028564725.2_OtoMan_p1.0_genomic.fna.gz`

6. Tools like GATK require an unzipped reference genome, so unzip the one you just downloaded using

    `gunzip GCA_028564725.2_OtoMan_p1.0_genomic.fna.gz`

Now your reference genome is on Della -- yay! 

There are other ways to get reference genomes. NCBI has a datasets tool -- this allows you to pull several different files all at once if you maybe wanted to pull the genomic sequence as well as the annotation file and RNA transcripts, etc. Also, maybe the reference genome you want isn't on NCBI. All fine -- get your genome from wherever you can, just make sure it's on Della so snpArcher can find it.

###
## (2) Pulling data from a sequence repository

You will probably have sequencing data you've generated that you want to map to this reference genome. You'll get instructions for downloading your sequencing data from whatever sequencing core sequenced your samples. 

You may also want to find some sequencing data generated by other people and published in some sequencing repository like NCBI, ENA, etc. More data can be helpful, because it increases the power of the mapping and calling algorithms that snpArcher uses.

There are several different approaches you might take for looking for published data. One approach is to just search on NCBI for published sequences in the Sequence Read Archive (SRA). If there is a lot of published data for your species, it might be easier to sort through by clicking on the "Send to Run Selector" link at the top of the SRA page for the species of interest. 

On Run Selector you can use the check boxes to select the specific runs you want to download. There may be several per sample or experiment, if some individuals were sequenced multiple times or different libraries were prepared. 

The data in SRA are laid out like
- SRP is Study, which can have ≥1 sample
- SRS is Sample, which can have ≥1 experiment
- SRX is Experiment, which can have ≥1 run
- SRR is Run. These are where the actual DNA sequences are!

Once you have some data you want to download, you can click the "Accession List" option under "Selected Data". This will give you a text file with a list of accession numbers for the runs you selected. 

From here you'll need to use the [SRA-toolkit](https://github.com/ncbi/sra-tools/wiki) to download the data. This is because NCBI stores sequencing data in this special SRA format. If the data you want are somewhere else, you might be able to use Globus to transfer data from this source to Della.

I store my sequencing data in another subdirectory called `raw_seq_data`, organized into further subdirectories by species or project.

###
## (3) Setting up the main snparcher environment

NOTE: the snpArcher [docs](https://snparcher.readthedocs.io/en/latest/setup.html) are good! They aren't super detailed, but they are readable. The github "issues" page is also an excellent resource if you ever run into problems. And, if you need to ask a question, the guys who are currently developing snpArcher seem really nice and helpful. 

The first thing the setup guide in the docs recommends is making a new snparcher conda environment. The current command is below, but check the docs to make sure these are still the versions of snakemake and python it wants you using

`mamba create -c conda-forge -c bioconda -n snparcher "snakemake>=8" "python==3.11.4"`

In Brian's tutorial he suggests then doing the following, to circumvent an error.

`mamba install -n snparcher -c conda-forge mamba`

If you've used snpArcher in the past, you won't need to do this again the next time you use it. 

Then, you'll need to clone the snpArcher github repo, which contains the pipeline and code. Do this with

`git clone https://github.com/harvardinformatics/snpArcher.git`

I've always cloned a new repo for each project. I think this helps prevent me from accidentally overwriting some outputs, and it means you're always getting the newest version of the pipeline when you run it, but it also means you'll have to do some of the setup steps below every time you reclone the repo. 

###
## (4) Configuring snpArcher  

You will need to get a couple of things set up prior to running snpArcher.

1. The most important is creating a samples.csv file, which you'll need to add to the `config` subdirectory. This gives the pipeline all of the metadata it needs to run. Some tips:

    - There is a script,  `workflow/snparcher_utils/write_samples.py` that can help you create this, but I find it's only suitable in a few situations. I almost always make mine at least semi by hand. 

    - See the docs for the required columns. You will want to specify absolute paths.

    - The SampleType column is optional, but you need to have it, and set it's value to `include` for all samples -- this is what generates the QC html at the end of the pipeline

    - I don't recommend changing the name of this file -- just call it `samples.csv` and stick it in the `config` subdirectory

2. The `config.yaml` file in this `config` subdirectory contains a lot of parameters you can adjust. Brian's workshop suggests just leaving most of these at the default. Here are some things you may want to change though:

    - `samples` -- the absolute path to your samples.csv 

    - `final_prefix` -- this is the prefix your results files will have

    - The pipeline was originally written for low coverage data, so the low coverage options will be selected by default. If you're working with data that has higher coverage than say, 10X, you can comment these lines out and uncomment the one for high coverage data

    - I remove the string in `scaffolds_to_exclude` 

    - You either need to set `generate_trackhub` to FALSE or specify your email address in trackhub_email

3. The biggest changes since Brian's workshop are in the slurm config file, which has been condensed (previously this info was in `resources.yaml` and the slurm `config.yaml`). You can find this "new" file at `profiles/slurm/config.yaml`. These updates added a lot of options -- this improved the pipeline a lot but also means there is more to do to get it set up.

    - You may want to set `retries` to something like 3. If any step errors out because of memory, the pipeline will resubmit that step with more memory and try again, up to the number of times you specify here. 
        
        - NOTE: This is very helpful, but can also cause you to waste a lot of time. snpArcher will keep resubmitting jobs even if you've not given it anywhere close to the resources needed to succeed. Keep an eye on the log file output, and check on any errors as the pipeline is running. If you see OOM errors, check that those same resubmitted rules aren't continuing to OOM error after subsequent retries. If you suspect you've set up your config to put snpArcher in a loop of submitting doomed jobs, you may want to kill your main job, wait for running processes to finish, change some things in the config, and resubmit.

    - I set `deafult resources` memory to 4000 (both `mem_mb` and `mem_mb_reduced`) and `runtime` to a day 1440 minutes. The higher you set this the less likely you are to fail steps because of timeouts or OOM errors. But, the higher you set this the longer your jobs will sit in the queue. 

    - For threads, I copied over the resources Brian specified in the old resources file and have made the following changes
        - genmap: 10
        - get_fastq_pe: 8 
        - fastp: 8
        - bwa_map: 20
        - dedup: 20
        - gvcf2DB: 2 
        - DB2vcf: 2
        - filterVcfs: 2
        - sort_gatherVcfs: 2
        - compute_d4: 4
    
    - You can set resources for specific rules after this. These will all come commented out as the default. You should leave anything you aren't changing commented. I made a lot of changes here. This is the part that took a lot of trial and error. 

       - To start you will need to uncomment the `set-resources:` line. 
       
       - If you want to change any resources for a rule, you need to uncomment the rule line, e.g. `index_reference:` and then whatever you're changing e.g. `mem_mb: attempt * 20000`. If you're not changing the slurm partition (you aren't, as we can't specify these on Della) or the runtime, leave these lines commented. 
       
       - The changes I made are too lengthy to detail here, but see the `slurm_config.yaml` file for what I've used in the past. You may need to tweak these depending on how much data you have, how big the genome is, how high coverage your data are etc. Note that this file should be named `config.yaml` and it should be in `profiles/slurm`. 

4. Be sure to save all of these files: `sample.csv`, `config/config.yaml` and `slurm/config.yaml` before running anything! 

###
## (5) More Della specific set up (script 01)

Copying the rest of this is pretty much directly from Brian, with only minor modifications. Brian provided us with three scripts:
1. One for setting up environments on Della
2. One for a dry run to test the rules snakemake thinks it needs to submit
3. One for actually submitting the main snparcher job. 

The versions on my repo have been modified by me to make them work after the updates. 

Okay, here's Brian:

Snakemake can create it's own mamba environments, and snpArcher automatically installs all the relevant software specified in `snpArcher/workflow/envs`. However, Della has a quirk in that compute nodes *do not* have access to internet, which is needed to download software. Thus, conda environments need to be created on login nodes, which *do* have internet, before we run anything.

1. In case it takes a while and you need to close your computer, let's make a 'screen' using `screen -S smk` that creates a seperate terminal instance

2. Now that we are in this separate screen, install conda environments using `bash 01_install_conda_envs.sh`. We may have to make some changes here, if someone runs this and can confirm it works/doesn't work, please let me know so I can update this page :)

3. Press `ctrl+A` then `D` to 'detatch' from the session, which should run in the background even if you close your terminal window.

4. To go back into the screen to check progess, type `screen -r smk`, where `-r` is to resume.

5. When it's finally done, kill the screen by pressing `ctrl+A` then `K`, and respond yes.

If you type `mamba env list`, you should see a bunch of new environments were created in a hidden directory `.snakemake` that you can only see using `ls -a`. These environments don't have names like the ones we created by hand.

NOTE: You could also do this directly on the command line on the login node, but using screen allows you to close your computer and it continues the process.

MADISON'S NOTE: You have to do this for every snpArcher directory you have. Again, I have separate ones for each of my projects. This means I have a lot of these environments in the hidden `.snakemake` directory. 
        
###
## (6) Dry run to test snpArcher (script 02)

Next, let's do a dry run to see if we have everything configured correctly.

`bash 02_dry_run.sh`

If the dry run completes successfully and shows some rules that need to be run, then everything is good to go. However, the number of rules is shows that need to be run isn't the entire story, since some rules are executed only under some conditions that are determined at runtime. In practice, snparcher will likely run thousands of rules and submit thousands of jobs.

###
## (7) Running snpArcher! (script 03)

Lastly, we can submit the last script `03_submit_smk_job.sh` as a job using the `sbatch` command, specifying the longest time interval possible to ensure it doesn't time out. This job that gets submitted will essentially submit many additional jobs, one for each rule that needs to get run. 

MADISON'S NOTE: I think the new version of snparcher will chastise you for submitting the main job as a slurm job, but to avoid using too many resources on the login node I'm not sure how else to do this. I know people on Argos just bash the main job, but I don't think that's an option for us.

Some more notes: 

- This may be a job to have an email set up to alert you if it errors out/finishes

- Sometimes, if you have a lot of data, this may take more than 6 days to run. Make sure to plan your submission around the annoying della downtimes. And, if you need to add more time you can write to cses@princeton.edu and politely ask them to add X amount more time to your main job (give them the job id). 

- Keep an eye on whats happening with `squeue -u YOUR_NETID` Make sure your jobs aren't languishing in the queue because you've asked for a lot of resources -- this slows you down! Note that sometimes snpArcher will be submitting jobs like crazy and sometimes you may see only one thing running. Both are normal, and more info about what it's doing now can be found in the error log

- Keep an eye on the error log. Scroll to the bottom to see recently submitted jobs. You can `CMD-F` and search for "error" to see if there have been any issues. You don't need to watch it like a hawk, but check in once or twice a day and see where you are in the pipeline and if any issue have arose.

###
## (8) A brief overview of the pipeline

It's probably important to understand the steps that get you from BAM to VCF so that you have a basic idea of what's going on at each step. The output of this multi-step process is a raw vcf file that you should consider filtering in different ways. As the SNParcher paper notes, this workflow is pretty much standard in the field. The default parameters are used in most of these steps, per GATK Best Practices.

### Step 1: Mapping
Sequence reads are mapped to the reference genome. The bwa-mem mapping stage DOES take into account mappability--how easy it would be to confidently map something to that part of the reference genome). Reads that align to regions of the genome with poor mappability (e.g. highly repetitive regions) will recieve lower mapping quality scores. These lower quality alignments are not removed, as far as I can tell, just annotated.

### Step 2: Calling Variants
After mapping, GATK is used to call variants for each sample's mapped reads. SNParcher uses HaplotypeCaller for this process. HaplotypeCaller is the recommended variant caller in almost all cases, according to GATK Best Practices, and I believe it's the only caller that SNParcher uses. HaplotypeCaller will call both SNPs and indels at the same time.

Nothing is filtered at this stage. Instead, calls are simply not made if there is not enough confidence for them. The SNParcher code for this step uses the GATK defaults for call confidence, which you can view [here, in the variant calling tutorial](https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/0471250953.bi1110s43). The output of this step is a gvcf, which includes reference confidence scores (support for the variant at that site), one for each sample. Parameters that also affect calls are `--min-pruning` and `--min-dangling-branch-length` which are semi-set in the config file. These are set when you decide whether to use the low-coverage pipeline (<10X) or the higher coverage one (>10X). 

Note that the process of calling variants occurs _per sample_. 

### Step 3: Calling Genotypes
The next step is to call genotypes and aggregate the gvcfs into a single vcf containing data from all samples. To make this efficient, SNParcher first uses GenomicsDBImport. I don't 100% understand what's going on here, but I don't think it matters too much. I think it's just organizing the computing work to be more efficient. 

GenotypeGVCFs is then used to do the actual calling. Again, at this step calls are just not made if there is not enough evidence to support them. Default call confidence thresholds are used (see the tutorial above, or ask chatGPT what they are). The one parameter that is used that you set is the prior probability of heterozygosity, which is set in the config file. The default value in the config (0.005) is higher that GATK's default (0.001). I don't really understand the implications of this, or why they made the decision to have this as the default.

Of note: variant calling is intentionally lenient. It aims to maximize sensitivity (the chance you will pick up on a variant) at the expense of accuracy (you will probably call some variants with little evidence to support them being "real"). While you may include some false positives through this process, you reduce the risk of throwing out true positives.

### Step 3: Annotating Called Variants with Filters
SNParcher then takes this vcf (which again, may have a lot of false positives!) and "filters" it. I think the language used in a lot of papers I've read describing the processing methods makes this step seem different from what actually happens. At this step, at least in the SNParcher pipeline, nothing is actually removed (removal is often referred to as "hard filtering", and snpArcher does this at a later step). Instead, you're just flagging variants that don't pass certain filters (ie, "soft filtering"). You can choose whether to hard filter them--actually remove them from your dataset--later. 

GATK Best Practices are used here. 
- **RPRS_filter**: flags variants supported by reads aligned in suboptimal positions or that have poor quality relative to their depth
    - for SNPs, `ReadPosRankSum` <-8.0 (for indels < -20.0)
    - for both SNPs and indels, anything with quality by depth (`QD`) < 2.0
- **FS_SOR_filter**: flags sites with strand bias (e.g. sequencing preferred the forward or reverse strand, so there is more skewed evidence for het or homozygosity compared to what you'd see if both strands were equally likely to be sequenced)
    - For SNPs, if Fisher Strand `FS` > 60.0 or Strand Odds Ratio `SOR` > 3.0
    - For indels or mixed sites, if `FS` > 200.0 or `SOR` > 10.0
- **MQ_filter**: annotates areas with low average mapping quality
    - For SNPs, if Mapping Quality `MQ` < 40.0 or `MQRankSum` < -12.5.
- **QUAL_filter**: annotates areas with low quality scores
    - `QUAL` less than 30.0

If a variant gets a filter flag it did not "pass" that filter. Variants with filter flags are ones you should consider removing, but again it's up to you to do this later. 

After all of this, you get a raw.vcf.gz with these flags on sites that didn't pass. 

###
## (9) Interpreting the outputs in the results directory

The main SNParcher pipeline gives you a final vcf file that ends with raw.vcf.gz. This vcf contains all of the samples you input and variants (SNPs and indels) called. 

If you set SampleType to "include" in your sample.csv, SNParcher will run its postprocesisng module. If you've already run the pipeline and identified samples that are very low coverage, contaiminated, etc. with the QC module, you can set those samples to "exclude" in the samples.csv and they will be filtered out in the postprocessing module. 

Here are the steps the postprocessing module performs:

### Step 1: Basic Filtering
At this step, SNParcher does three things:

1. removes those samples that you told it to exclude in the sample.csv file, if any

2. hard filters out sites that were flagged above (ie, sites that don't have a . or PASS in the FILTER COLUMN)

    - in my vcf, no sites are marked with PASS. You can check the FILTER.summary in results for a summary of what was marked. I also ran the following on the command line to confirm:

    `zgrep -v "^#" 202405_AENP_GNP_NCBI_raw.vcf.gz | head -n 10000 | awk -F'\t' '$7 == "PASS" {count++} END {print count+0}'` 
    
    This printed 0 for me.

    `zgrep -v "^#" 202405_AENP_GNP_NCBI_raw.vcf.gz | head -n 10000 | awk -F'\t' '$7 == "MQ_filter" {count++} END {print count+0}'`

     You should get something >0 for this one (unless your dataset is super good I guess?)

    `zgrep -v "^#" 202405_AENP_GNP_NCBI_raw.vcf.gz | head -n 10000 | awk -F'\t' '$7 == "." {count++} END {print count+0}'`
    
    Should definitely be >0, these are the unlabelled PASSes

3. removes from the vcf any sites that are not polymorphic, or where the ref genome has an unknown (N) base

    - sites that don't have any variation (all samples have the same sequence)

    - sites where the reference has an N or where the alternate is . (no alternate allele)

    - sites with allele frequency = 0 (no alternate alleles observed)

The output here is the filtered.vcf.gz file.

### Step 2: Strict Filtering
Then SNParcher will use bcftools to filter according to the parameters you specify in the config file, and split the vcf into one for SNPs and one for indels. 

1. excludes variants in regions of the genome comprised of small contigs 

2. removes variants with "missingness" above the threshold specified in the config.yaml

3. removes variants with a minor allele frequency below the threshold specified in the config.yaml

4. removes variants on chromosomes you specify in the config.yaml

The output of this is a TEMP file that is then just subsetted in the next snakemake rules. This TEMP file gets split into a SNP file and an indel file. Wish they had an option to keep the TEMP file with them together but oh well! 




